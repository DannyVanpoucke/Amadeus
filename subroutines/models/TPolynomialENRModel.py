# -*- coding: utf-8 -*-
"""
Created on Mon Nov 11 15:08:20 2019

Polynomial Regression with Elastic Net Regularization

@author: Dr. Dr. Danny E. P. Vanpoucke
@web   : https://dannyvanpoucke.be
"""
import pandas as pd
from TModelClass import TModelClass
import numpy as np
from TModelResults import TModelResults
from Bootstrap import TBootstrap
from TModelQualityData import TModelQualityData
from sklearn.pipeline import Pipeline

from sklearn.exceptions import ConvergenceWarning


class TPolynomialENRModel(TModelClass):
    """
    Child class representing the polynomial regression model, using Elastic Net Regularisation
    """
    def __init__(self,name,Target, Feature: pd.DataFrame, 
                 Target_test, Feature_test: pd.DataFrame,
                 Pipeline: Pipeline,
                 EnsemblePipeline: Pipeline,
                 Degree: int=2, Interaction: bool=False, 
                 Bias: bool=True, N_Mix: int=10, CrossVal: int=5, MixVal: float=0.5):
        """
        Constructor to set up a polynomial model with Elastic Net Regularisation. The Alpha 
        and R values are obtained via cross-validation. For each Mixing value, 100 alpha 
        values are checked.
    
        parameters:
         - name : the name of the object instance
         - Feature : The features to use & transform
         - Target     : the training target data
         - Target_test: the test target data
         - Feature_test: the untransformed features for testing.
         - Pipeline : a pipeline generated by the PipelineFactory
         - EnsemblePipeline : the pipeline generated by the PipelineFactory for the entire ensemble dataset
         - Degree  : The polynomial degree (integer), DEFAULT=2
         - Interaction : Boolean variable indicating if ONLY interaction terms are included in the 
                     polynomial features (True) or that all terms are included (False), DEFAULT=False
         - Bias    : Boolean indicating if a bias column is added (i.e. where all powers 
                      are zero, acts as intercept in linear model). [DEFAULT = True]
         - N_Mix   : Number of mixing ratios between Ridge and Lasso Regression to investigate. 
                 float: 0..1.0  [DEFAULT = 10]
                 Values are generated from 0.01 to 1, with 0.1 and 1 the outer borders, unless N_Mix<2, 
                 in which case 0.5 is set as only value, or the value supplied in MixVal. No values larger 
                 than 40 are accepted. If provided, N_Mix is set to 40
         - CrossVal: How-many-fold crossvalidation is required? [DEFAULT = 5]
         - MixVal : If only one mixing value is chosen, then this will be it. [DEFAULT = 0.5]
     
        It sets the following properties
            - pipeline : a pipeline object containing the preprocessing transformations (excluding the fitter function)
            - CVmodel  : the fitter function to be used (should be an sklearn function with "fit" method)
            - model : The model is only set once the CV_model has run a fit-operation
            - feature_tf: the transformed features as obtained by the pipeline  
            
            - parJobs : number of cores to use. Go full parallel on serial case, and single core on parallel code
        parameters sets for accounting/tracking purposes
            - crossVal : the Crossvalidation model/size (=CrossVal parameter) 
            - bestAlpha: best alpha-hyperparameter as obtained by ElasticNetCV
            - bestL1_ratio: best l1_ratio-hyperparameter as obtained by ElasticNetCV
            - cv_iter : number of iterations needed by ElasticNetCV
            - cv_warns: %-number of convergence-warnings thrown during ElasticNetCV = #warnings/(#alphas * #l1_ratio)
            
        """
        #from sklearn.preprocessing import StandardScaler
        #from sklearn.preprocessing import PolynomialFeatures
        from sklearn.linear_model import ElasticNetCV
        from multiprocessing import current_process
        import numpy as np
    
        super().__init__(name,Target, Feature,Target_test, Feature_test)
        if current_process().name == 'MainProcess':
            print('Hello from the main process')
            self.parJobs=-1
        else:
            print('Hello from child process')
            self.parJobs=1
        
        
        self.nameModel='Polynomial Model with Elastic Net Regularisation'
        self.name=name
        print("Initialising the child class:",self.nameModel)
        #create a pipeline (can be extended to contain more functions, p67)
        self.pipeline = Pipeline
        self.EnsPipe = EnsemblePipeline
#        self.pipeline = Pipeline([
#            ('poly_features',PolynomialFeatures(degree=Degree,interaction_only=Interaction ,include_bias=Bias)),# polynamial terms up to degree 3, and no bias column (this would be intercept in case of linear fit)
#            ('std_scaler', StandardScaler(with_mean=False, with_std=False)),#a standard scaler which does nothing
#            #('std_scaler', StandardScaler()),  #scaling to be centered on 0, with unit variance...since the values are quite different, this will help things
#        ]) #don't include the fitter
        
        self.feature_tf = self.pipeline.fit_transform(Feature) #this is a numpy array...
                
        #setup the list of possible mixing ratios
        if N_Mix>40:
            N_Mix=40
        #NOTE THAT l1_ratio=0 crashes sklearn as it can not generate the needed alphas
        MixLst=[]
        if N_Mix<2:
            MixLst.append(MixVal)
        elif N_Mix == 2:
            MixLst.append(0.01)
            MixLst.append(1.0)
        else:
            #an exponential distribution of points: Pm=ln(1+m(e-1)/N) for point m of N
            for i in range(1,N_Mix+1):
                Step=np.log(1.0 + i*(np.exp(1)-1.0)/N_Mix)
                MixLst.append(Step)
            
        #track the r1 and alpha lists for the CV model
        self.l1lst=MixLst
        self.n_alphas=100 # 100 is the sklearn default
        self.CVmodel = ElasticNetCV(l1_ratio=self.l1lst, #test generated list of mixing
                         n_alphas=self.n_alphas,        #test 100(default) alphas
                         fit_intercept=True, #the data is already centered by standard scaler
                         normalize=False,
                         precompute='auto',   #let sklearn decide if the Gram matrix needs precomputing
                         max_iter = 1000,
                         tol = 1.0E-3,
                         cv=CrossVal,         #X-fold crossvalidation--> for our small data: leave-one-out?
                         n_jobs=self.parJobs  #if you use -1 (= go parallel over all cores), you can not suppress the annoying convergence warning
                        )
        #to keep track of some options:
        self.crossVal=CrossVal
        self.bestAlpha=0
        self.bestL1_ratio=0
        self.cv_iter=0
        self.cv_warns=0 # %-number of warnings thrown during ElasticNetCV = #warnings/(#alphas * #l1_ratio)
        self.model=None
    
    #@ignore_warnings(category=ConvergenceWarning)    
    def fit(self):
        """
        Class-method wrapping the fit-method of the sklearn model
           - Target : a pandas dataframe with the Target data belonging to the 
                   Features provided upon initialisation.
        """
        import warnings
        from sklearn.linear_model import ElasticNet
        
        with warnings.catch_warnings(record=True) as caught_warnings:
            warnings.simplefilter("always",category=ConvergenceWarning)
            self.CVmodel.fit(self.feature_tf,self.target)
        warnCount=len(caught_warnings)
        
        totcnt=sum(len(x) for x in self.CVmodel.alphas_)
        self.cv_warns=(warnCount/totcnt)*100.0
        #keep track of the best alpha and mixing, and use these to set up the actual model
        self.bestAlpha=self.CVmodel.alpha_
        self.bestL1_ratio=self.CVmodel.l1_ratio_
        self.cv_iter=self.CVmodel.n_iter_
        self.model=ElasticNet(alpha=self.bestAlpha,
                              l1_ratio=self.bestL1_ratio,
                              fit_intercept=True, #the data is already centered by standard scaler...but that seems not to be relevant/ or what is meant by this
                              normalize=False,
                              precompute=True     #precompute the Gram matrix (default)
                             )
        print("=== MODEL WAS OPT HERE : n-alpha=",self.n_alphas, " #WARN-->",warnCount,'/',totcnt," : ",self.cv_warns," % ================")
        #print(na,": #WARN-->",warnCount,'/',totcnt," : ",self.cv_warns," % | a=",self.bestAlpha," r1=",self.bestL1_ratio)
        #print(" %5i : #WARN--> %5i / %5i : %9.4f %%  | a=  %9.4f  r1=  %9.4f "%(na,warnCount,totcnt,self.cv_warns,self.bestAlpha,self.bestL1_ratio) )
        
            
            
            
            
        #from sklearn.model_selection import RandomizedSearchCV
        #from scipy.stats import uniform, expon
        #import numpy as np
        #distributions=dict(alpha=expon(), l1_ratio=uniform(loc=0,scale=1))
        #clf=RandomizedSearchCV(self.model, distributions, n_iter=1000, 
        #                       scoring='neg_mean_squared_error',
        #                       cv=self.crossVal,
        #                       random_state=0,
        #                       error_score=np.nan,
        #                       return_train_score=True
        #                       )
        #with warnings.catch_warnings(record=True) as caught_warnings:
        #    warnings.simplefilter("always",category=ConvergenceWarning)
        #    search=clf.fit(self.feature_tf,self.target)
        #warnCount=len(caught_warnings)
        #print("RandomSearch 1000 WARNINGS: ",warnCount,'/',clf.n_iter)
        #print("best parameters Random search:", search.best_params_)
        
        print("--------------------------------")
        print("best alpha=",self.bestAlpha)
        print("best L1_rat=",self.bestL1_ratio)

        self.model.fit(self.feature_tf,self.target)
        self.setCoefficients()
        print("did some fitting, Parent-style:",type(self.model).__name__)
        
    def fitSanityCheck(self)->int:
        """
        Class method which should cover/deal with failures of sklearn.
        
        For some reason, sklearn LinearRegression randomly fails on small datasets.
        This failure gives rise to huge coefficents. Hoever, just shuffling the 
        data seems to resolve the issue.
        
        This function returns the number of shuffles needed to regain sanity.
        """
        import sys
        from sklearn.linear_model import ElasticNetCV

        #If we had too many warnings then something probably went wrong
        #start while loop untill it works...or n_alphas becomes too big?
        cnt=0
        thress=5.0
        insane=(self.cv_warns>thress) #larger than 5% warnings
        
        while (insane and (self.n_alphas<10000)): #more than 5% warnings, and less than 10K alphas
            cnt+=1
            fac=(self.cv_warns/thress)*1.50
            if fac>10.0: #limit it to max 10
                fac=10.0
            self.n_alphas=int(self.n_alphas*fac)
            self.cv_warns=0
            self.CVmodel = ElasticNetCV(l1_ratio=self.l1lst, #test generated list of mixing
                         n_alphas=self.n_alphas,        #test 100 alphas
                         fit_intercept=True, #the data is already centered by standard scaler
                         normalize=False,
                         precompute='auto',   #let sklearn decide if the Gram matrix needs precomputing
                         max_iter = 1000,
                         tol = 1.0E-3,
                         cv=self.crossVal,         #X-fold crossvalidation--> for our small data: leave-one-out?
                         n_jobs=self.parJobs  #if you use -1 (= go parallel over all cores), you can not suppress the annoying convergence warning
                        )
            self.fit()
            insane=(self.cv_warns>thress)
        
        if (cnt>0):#update the coefficients
            self.setCoefficients()
        
        if insane:
            print("EPIC FAIL, n_alphas=10K is just crazy. Attempts at sanity failed in the ",self.name,". Terminating this sick job!")
            sys.exit()
        
        return cnt

##serial
#    def setAverageCoefficients(self, EnsembleData: TModelResults, setCI: bool):
#        """
#        Use the ensemble data to create an "average" model, and set the "coefficients"
#        in the current model. This should be performed in each model separately
#        
#        --> needs to include hyper paramaters...how do we deal with multi-preference?
#        
#        """
#        from sklearn.linear_model import ElasticNet
#        #import time
#        
#        # 1. Calculate the average coefficients
#        # 1.1. transform them to arrays
#        #start = time.perf_counter_ns()
#        #print("3.1) Average Coefficients : AVG")
#        intercept=np.zeros(EnsembleData.NData)
#        coef=np.zeros((EnsembleData.NData,EnsembleData.modelCoef[0]['coef_'][1].shape[1]))
#        for i in range(EnsembleData.NData):
#            mcf=EnsembleData.modelCoef[i]
#            intercept[i]=np.asarray(mcf['intercept_'][1]).ravel()
#            coef[i,:]=np.asarray(mcf['coef_'][1]).ravel()
#            print(i,")",coef[i,:])
#            
#        mean_intercept=np.mean(intercept,axis=0)#axis is the varying direction, so 0 means we calculate the average of a column by varying the row
#        mean_coef=np.mean(coef,axis=0) 
#        print("MEANS=> Int=",mean_intercept,"  COEF=",mean_coef)
#        # 2. Set the model coefficients to these averaged values
#        # ENR and sklearn black-boxing complicate things a bit here:
#        self.model=ElasticNet(alpha=1.0,     # temp values -> will be mean?
#                              l1_ratio=0.5,  # temp values -> will be mean?
#                              fit_intercept=True, #the data is already centered by standard scaler...but that seems not to be relevant/ or what is meant by this
#                              precompute=True     #precompute the Gram matrix (default)
#                             )
#       #self.model.fit(self.feature_tf,self.target) #just to make sure the intercept and coef attributes are defined--> fuck python 
#        
#        
#        
#        
#        self.model.intercept_=mean_intercept
#        self.model.coef_=mean_coef
#        self.isAverage = True
#        self.hasCI=False
#        if setCI:
#            #end = time.perf_counter_ns()
#            #print("3.2.a) Average Coefficients : CI Intercept ",(end-start)/10E9)
#            # 3. Calculate Confidence Interval using Bootstrapper tech?
#            # & 4. Store the CI data
#            ## For the intercept
#            boot=TBootstrap(data=intercept,Func=np.mean)
#            #end = time.perf_counter_ns()
#            #print("3.2.b) NPboot",(end-start)/1E9)
#            boot.NPbootstrap(n_iter=2000, Jackknife=True)
#            #end = time.perf_counter_ns()
#            #print("3.2.c) Con Int",(end-start)/1E9)
#            avgm, avgp = boot.ConfidenceInterval(CItype="BCa",alpha=0.05,n_samples=2000)#95%confidence interval
#            self.CI["intercept_lo"]=avgm
#            self.CI["intercept_hi"]=avgp
#            ## For the coefficients
#            avgml=list()
#            avgpl=list()
#            for col in range(EnsembleData.modelCoef[0]['coef_'][1].shape[1]):
#                #end = time.perf_counter_ns()
#                #print("3.2) Average Coefficients : CI Coef ",col," ",(end-start)/1E9)
#                boot=TBootstrap(data=coef[:,col],Func=np.mean)
#                boot.NPbootstrap(n_iter=2000, Jackknife=True)
#                avgm, avgp = boot.ConfidenceInterval(CItype="BCa",alpha=0.05)#95%confidence interval
#                avgml.append(avgm)
#                avgpl.append(avgp)
#                
#            self.CI["coef_lo"]=avgml
#            self.CI["coef_hi"]=avgpl
#            self.hasCI = True
#            
#        #store the resulting coefficients in our wrapper tracker...and we are done
#        self.setCoefficients()
#        self.Quality=TModelQualityData(EData=EnsembleData)
  
    #Important note: onle one pre-underscore is allowed, otherwise python does not find this function
    def _BootstrapAvg_1Col(self,col:int, coeflst:list, alpha:float)->tuple:
        """
        Single line parallellizable bootstrap for a column of coefficients.
        
        parameters:
            - col: the index of the column (for administrative purposes upon return)
            - coeflst: the list of coefficients
            - alpha: the BCa alpha value for the CI, default=0.05
        return:
            tuple(col-index, CIlow, CIhigh)
        """
        #print("$$$$$$$$$$$$$$$ boot1 col=",col)
        boot=TBootstrap(data=coeflst,Func=np.mean)
        boot.NPbootstrap(n_iter=2000, Jackknife=True)
        avgm, avgp = boot.ConfidenceInterval(CItype="BCa",alpha=alpha)#95%confidence interval
        
        return tuple([col,avgm,avgp])
    
#parallel
    def setAverageCoefficients(self, EnsembleData: TModelResults, setCI: bool):
        """
        Use the ensemble data to create an "average" model, and set the "coefficients"
        in the current model. This should be performed in each model separately
        
        --> needs to include hyper paramaters...how do we deal with multi-preference?
        
        """
        from sklearn.linear_model import ElasticNet
        import multiprocessing as mp
        from HPCTools import get_num_procs
        
        # 1. Calculate the average coefficients
        # 1.1. transform them to arrays
        #start = time.perf_counter_ns()
        #print("3.1) Average Coefficients : AVG")
        intercept=np.zeros(EnsembleData.NData)
        coef=np.zeros((EnsembleData.NData,EnsembleData.modelCoef[0]['coef_'][1].shape[1]))
        for i in range(EnsembleData.NData):
            mcf=EnsembleData.modelCoef[i]
            intercept[i]=np.asarray(mcf['intercept_'][1]).ravel()
            coef[i,:]=np.asarray(mcf['coef_'][1]).ravel()
            print(i,")",coef[i,:])
            
        mean_intercept=np.mean(intercept,axis=0)#axis is the varying direction, so 0 means we calculate the average of a column by varying the row
        mean_coef=np.mean(coef,axis=0) 
        print("MEANS=> Int=",mean_intercept,"  COEF=",mean_coef)
        # 2. Set the model coefficients to these averaged values
        # ENR and sklearn black-boxing complicate things a bit here:
        self.model=ElasticNet(alpha=1.0,     # temp values -> will be mean?
                              l1_ratio=0.5,  # temp values -> will be mean?
                              fit_intercept=True, #the data is already centered by standard scaler...but that seems not to be relevant/ or what is meant by this
                              normalize=False,
                              precompute=True     #precompute the Gram matrix (default)
                             )
       #self.model.fit(self.feature_tf,self.target) #just to make sure the intercept and coef attributes are defined--> fuck python 
        
        self.model.intercept_=mean_intercept
        self.model.coef_=mean_coef
        self.isAverage = True
        self.hasCI=False
        if setCI:
            #end = time.perf_counter_ns()
            #print("3.2.a) Average Coefficients : CI Intercept ",(end-start)/10E9)
            # 3. Calculate Confidence Interval using Bootstrapper tech?
            # & 4. Store the CI data
            ## For the intercept
            boot=TBootstrap(data=intercept,Func=np.mean)
            #end = time.perf_counter_ns()
            #print("3.2.b) NPboot",(end-start)/1E9)
            boot.NPbootstrap(n_iter=2000, Jackknife=True)
            #end = time.perf_counter_ns()
            #print("3.2.c) Con Int",(end-start)/1E9)
            avgm, avgp = boot.ConfidenceInterval(CItype="BCa",alpha=0.05,n_samples=2000)#95%confidence interval
            self.CI["intercept_lo"]=avgm
            self.CI["intercept_hi"]=avgp
            ## For the coefficients
             ## For the coefficients
            # Parallelisation for sections performing bootstraps. 
            # Parallelization at the highest level of a column, 
            # ??Is the overhead sufficiently low to have benefits?
            # 1. create our process pool with as many processes as physical cores
            pool=mp.Pool(processes=get_num_procs(-1))
            # 2. set drones to work
            alpha=0.05 #95%confidence interval
            drones=[pool.apply_async(self._BootstrapAvg_1Col, args=(col,coef[:,col],alpha)) for col in range(EnsembleData.modelCoef[0]['coef_'][1].shape[1])]
            # 3. as we can not assume the cols to be produced in the correct order
            #    --> make it a dict
            ciDict=dict()
            for drone in drones:
                col,avgm,avgp=drone.get()
                ciDict[col]=list([avgm,avgp])
            # 4. wait untill all processes are finished
            pool.close()
            pool.join()
            # 5. and put then in the corrcet order in the list        
            avgml=list()
            avgpl=list()
            for col in range(EnsembleData.modelCoef[0]['coef_'][1].shape[1]):
                avgml.append(ciDict[col][0])
                avgpl.append(ciDict[col][1])
            
            self.CI["coef_lo"]=avgml
            self.CI["coef_hi"]=avgpl
            self.hasCI = True
            
        #store the resulting coefficients in our wrapper tracker...and we are done
        self.setCoefficients()
        self.Quality=TModelQualityData(EData=EnsembleData)

    
    def printAverageCoefficients(self, File: str=None):
        """
        Print a block of information to a file, containing the averaged coefficients for the 
        polynomial model.
        
        --> needs to include hyper paramaters...
        
        
        parameters:
            - self:
            - File: string containing a filename, if None standard output is used. Default=None
        """
        coefstr=list()
        
        pipeline=None
        for name, step in self.EnsPipe.steps:
            if (name == 'poly_features'):
                pipeline=self.EnsPipe
        if pipeline==None:
            for name, step in self.pipeline.steps:
                if (name == 'poly_features'):
                    pipeline=self.pipeline
        
        
        for i in range(pipeline['poly_features'].powers_.shape[0]):
            line="("
            for j in range(pipeline['poly_features'].powers_.shape[1]):
                pw=pipeline['poly_features'].powers_[i,j]
                if (pw != 0):
                    line=line+"x_"+str(j)+"^"+str(pw)
            line=line+")"
            if len(line)==2: #so this term is missing
                line="(1)"
            coefstr.append(line)
        
        if File is None:
            print("======= THE AVERAGED MODEL ==============")
            print(" Model : ",self.name)
            print(self.Quality.QualitiesText())
            if self.hasCI:
                print("Intercept  : ",self.model.intercept_," and CI=[",self.CI["intercept_lo"]," ; ",self.CI["intercept_hi"],"]")
                for col in range(len(self.model.coef_)):
                    print("coef ",coefstr[col]," : ",self.model.coef_[col]," and CI=[",self.CI["coef_lo"][col]," ; ",self.CI["coef_hi"][col],"]")
            else:
                print("Intercept  : ",self.model.intercept_)
                for col in range(len(self.model.coef_)):
                    print("coef ",coefstr[col]," : ",self.model.coef_[col])
            print("====================================\n\n")
        else:
            foo=open(File,"a+",)
            foo.write("======= THE AVERAGED MODEL ==============\n")
            line=" Model : "+self.name+"\n"
            foo.write(line)
            foo.write(self.Quality.QualitiesText())
            if self.hasCI:
                line="Intercept  : "+str(self.model.intercept_)+" and CI=["+str(self.CI["intercept_lo"])+" ; "+str(self.CI["intercept_hi"])+"] \n"
                foo.write(line)
                for col in range(len(self.model.coef_)):
                    line="coef "+coefstr[col]+" : "+str(self.model.coef_[col])+" and CI=["+str(self.CI["coef_lo"][col])+" ; "+str(self.CI["coef_hi"][col])+"] \n"
                    foo.write(line)
            else:
                line="Intercept  : "+str(self.model.intercept_)+"\n"
                foo.write(line)
                for col in range(len(self.model.coef_)):
                    line="coef "+coefstr[col]+" : "+str(self.model.coef_[col])+"\n"
                    foo.write(line)
            foo.write("====================================\n\n")
            foo.close() 
    
    
    
               
    def setCoefficients(self):
        """
        Class-method printing the fitting coefficients for a polynomial regression 
        with elastic net regularization
        """
        import numpy as np
        super().setCoefficients()
        #--------- hyper parameters -------------------
        self.modelcoef['header_hyperparameter']=[self.coefindex,"The selected best alpha and mixing hyper-parameters for Elastic Net Regularization are:"]
        line="  - alpha    = %0.3f  " % (self.bestAlpha )
        self.modelcoef['alpha']=[-(self.coefindex+1),line]
        line="  - l1_ratio = %0.3f  " % (self.bestL1_ratio )
        self.modelcoef['l1_ratio']=[-(self.coefindex+2),line]
        line="  - n_iter_CV = %i  " % (self.cv_iter )
        self.modelcoef['n_iter']=[-(self.coefindex+3),line]
        line="  - CV-warnings = %0.2f  %%" % (self.cv_warns )
        self.modelcoef['cv_warns']=[-(self.coefindex+4),line]
        
        #------------usual coefficients-----------------
        self.modelcoef['header_coef']=[self.coefindex+5,"The coefficients for each target (one per row) are given by:"]
        self.modelcoef['coef_']=[self.coefindex+6,np.array([self.model.coef_])]
        self.modelcoef['header_intercept']=[self.coefindex+7,"The intercepts for each target (one per row) are given by:"]
        self.modelcoef['intercept_']=[self.coefindex+8,np.array([self.model.intercept_])]
        self.coefindex+=9
        
        